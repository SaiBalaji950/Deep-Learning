{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f936e0-9187-4abb-a2c4-4aa8ce199f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch\n",
    "\n",
    "Pytorch is a small part of computer software which is based on the torch Library.\n",
    "It is a deep learning Framework and machine learning library for python programming\n",
    "language which is used for the applications such as Natural Language Processing(NLP).\n",
    "\n",
    "It was introduced by facebook.\n",
    "\n",
    "The high-level features which are provided by PyTorch are as follows:\n",
    "\n",
    "With the help of the Graphics Processing Unit (GPU), it gives tensor computing with strong acceleration.\n",
    "It provides Deep Neural Network which is built on a tape-based auto diff system.\n",
    "PyTorch was developed to provide high flexibility and speed during implementing \n",
    "and building the Deep Learning Neural Network. As you already know, it is a machine learning library \n",
    "for Python programming language, so it's quite simple to install, run, and understand. \n",
    "Pytorch is completely pythonic (using widely adopted python idioms rather than writing Java and C++ code) \n",
    "so that it can quickly build a Neural Network Model successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e884822-538e-4c33-84d8-d803dca8d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The high-level features which are provided by PyTorch are as follows:\n",
    "\n",
    "With the help of the Graphics Processing Unit (GPU), it gives tensor computing with strong acceleration.\n",
    "It provides Deep Neural Network which is built on a tape-based auto diff system.\n",
    "PyTorch was developed to provide high flexibility and speed during implementing and building the Deep Learning Neural Network. \n",
    "As you already know, it is a machine learning library for Python programming language, \n",
    "so its quite simple to install, run, and understand. \n",
    "Pytorch is completely pythonic (using widely adopted python idioms rather than writing Java and C++ code) \n",
    "so that it can quickly build a Neural Network Model successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f94c66-dcff-4992-baf3-ed53313d79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "History of PyTorch\n",
    "\n",
    "PyTorch was released in 2016. Many researchers are willing to adopt PyTorch increasingly. \n",
    "It was operated by Facebook. Facebook also operates Caffe2 (Convolutional Architecture for Fast Feature Embedding). \n",
    "It is challenging to transform a PyTorch-defined model into Caffe2. \n",
    "For this purpose, Facebook and Microsoft invented an Open Neural Network Exchange (ONNX) in September2017. \n",
    "In simple words, ONNX was developed for converting models between frameworks. Caffe2 was merged in March 2018 into PyTorch.\n",
    "\n",
    "PyTorch makes ease in building an extremely complex neural network. This feature has quickly made it a go-to library. \n",
    "In research work, it gives a tough competition to TensorFlow. Inventors of PyTorch wants to make a highly imperative library \n",
    "which can easily run all the numerical computation, and finally, they invented PyTorch. There was a big challenge for Deep learning scientist, \n",
    "Machine learning developer, and Neural Network debuggers to run and test part of the code in real-time. \n",
    "PyTorch completes this challenge and allows them to run and test their code in real-time. So they dont have to wait to check whether it works or not.\n",
    "\n",
    "Note: To use the PyTorch functionality and services, you can use Python packages such as NumPy, SciPy, and Cython.\n",
    "Why use PyTorch?\n",
    "Why PyTorch? \n",
    "What is special in PyTorch which makes it special to build Deep learning model. \n",
    "PyTorch is a dynamic library. Dynamic library means a flexible library, and you can use that library as per your requirements and changes. \n",
    "At present in Kaggle competition, it is continuously used by finishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdf5ba-40aa-4b6f-9daa-b5115fdd7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features of Pytorch\n",
    "\n",
    "1) Simple Interface\n",
    "2) Hybrid Front-End\n",
    "3) Distributed Training\n",
    "4) Python First\n",
    "5) Tools and Libraries\n",
    "6) Native ONNX(Open Neural Network Support) Support\n",
    "7) C++ Front End\n",
    "8) Cloud Partners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b95c7b-8620-4bdb-88a4-e38f5937f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Installation of Pytorch\n",
    "\n",
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f60566-76ef-48d5-a37d-e76132086047",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ee68e-886e-42a2-83ec-3de225ed04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor\n",
    "torch.dtype\n",
    "torch.device\n",
    "torch.layout\n",
    "torch.finfo\n",
    "torch.iinfo\n",
    "torch.sparse\n",
    "torch.cuda\n",
    "torch.storage\n",
    "torch.nn\n",
    "torch.nn.functional\n",
    "torch.optim\n",
    "torch.autogard\n",
    "torch.distributed\n",
    "torch.distribution\n",
    "torch.hub\n",
    "torch.multiprocessing\n",
    "torch.utils.bottleneck\n",
    "torch.utils.checkpoint\n",
    "torch.tils.cpp_extension\n",
    "torch.utils.data\n",
    "torch.utils.dlpack\n",
    "torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7d100-6bc3-4996-92f3-76cd6a27c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch Packages\n",
    "PyTorch is an optimized tensor library for deep learning using CPUs and GPUs. \n",
    "PyTorch has a rich set of packages which are used to perform deep learning concepts. \n",
    "These packages help us in optimization, conversion, and loss calculation, etc. Let's get a brief knowledge of these packages.\n",
    "\n",
    "1.\n",
    "Torch\n",
    "The torch package includes data structure for multi-dimensional tensors and mathematical operation over these are defined.\n",
    "2.\n",
    "torch.Tensor\n",
    "This package is a multi-dimensional matrix which contains an element of a single data type.\n",
    "3.\n",
    "Tensor Attributes\n",
    "a) torch.dtype\n",
    "It is an object which represents the datatype of thetorch.Tensor.\n",
    "b) torch.device\n",
    "It is an object that represents the device on which torch.Tensor will be allocated.\n",
    "c) torch.layout\n",
    "It is an object which represents a memory layout of a toch.Tensor.\n",
    "4.\n",
    "Type Info\n",
    "The numerical properties of a torch.dtype will be accessed through either the torch.iinfo or the torch.finfo.\n",
    "1) torch.finfo\n",
    "It is an object which represents the numerical properties of a floating-point torch.dtype.\n",
    "2) torch.iinfo\n",
    "It is an object which represents the numerical properties of an integer torch.dtype.\n",
    "5.\n",
    "torch.sparse\n",
    "Torch supports sparse tensors in COO (rdinate) format, which will efficiently store and process tensors for which the majority of elements are zero.\n",
    "6.\n",
    "torch.cuda\n",
    "Torch supports for CUDA tensor types which implement the same function as CPU tensors, but for computation they utilize GPUs.\n",
    "7.\n",
    "torch.Storage\n",
    "A torch.Storage is a contiguous, one-dimensional array of a single data type.\n",
    "8.\n",
    "torch.nn\n",
    "This package provides us many more classes and modules to implement and train the neural network.\n",
    "9.\n",
    "torch.nn.functional\n",
    "This package has functional classes which are similar to torch.nn.\n",
    "10.\n",
    "torch.optim\n",
    "This package is used to implement various optimization algorithm.\n",
    "11.\n",
    "torch.autogard\n",
    "This package provides classes and functions to implement automatic differentiation of arbitrary scalar value functions.\n",
    "12.\n",
    "torch.distributed\n",
    "This package supports three backends and each one is with different capabilities.\n",
    "13.\n",
    "torch.distribution\n",
    "This package allows us to construct the stochastic computation graphs, and stochastic gradient estimators for optimization\n",
    "14.\n",
    "torch.hub\n",
    "It is a pre-trained model repository which is designed to facilitate research reproducibility.\n",
    "15.\n",
    "torch.multiprocessing\n",
    "It is a wrapper around the native multiprocessing module.\n",
    "16.\n",
    "torch.utils.bottleneck\n",
    "It is a tool which can be used as an initial step for debugging bottlenecks in our program.\n",
    "17.\n",
    "torch.utils.checkpoint\n",
    "It is used to create checkpoint in our source program.\n",
    "18.\n",
    "torch.tils.cpp_extension\n",
    "It is used to create the extension of C++, CUDA, and other languages.\n",
    "19.\n",
    "torch.utils.data\n",
    "This package is mainly used for creating the dataset.\n",
    "20.\n",
    "torch.utils.dlpack\n",
    "It will use to decode the Dlpack into tensor.\n",
    "21.\n",
    "torch.onnx\n",
    "The ONNX exporter is a trace-based exporter, which means that it operates by executing your model once and \n",
    "exporting the operators which were actually run during this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeedc24-c9b2-4063-8b37-7ddfd76ca3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch nn paramters\n",
    "\n",
    "torch.nn.paramters\n",
    "\n",
    "\n",
    "Containers\n",
    "\n",
    "torch.nn.Module\n",
    "torch.nn.Sequential\n",
    "torch.nn.ModuleList\n",
    "torch.nn.ModuleDict\n",
    "torch.nn.ParameterList\n",
    "torch.nn.ParameterDict\n",
    "\n",
    "\n",
    "Convolutional Layers\n",
    "\n",
    "torch.nn.Conv1d\n",
    "torch.nn.Conv2d\n",
    "torch.nn.Conv3d\n",
    "torch.nn.ConvTranspose1d\n",
    "torch.nn.ConvTranspose2d\n",
    "torch.nn.ConvTranspose3d\n",
    "torch.nn.Unfold\n",
    "torch.nn.Fold\n",
    "\n",
    "\n",
    "Pooling Layers\n",
    "\n",
    "torch.nn.MaxPool1d\n",
    "torch.nn.MaxPool2d\n",
    "torch.nn.MaxPool3d\n",
    "torch.nn.MaxUnpool1d\n",
    "torch.nn.MaxUnpool2d\n",
    "torch.nn.MaxUnpool3d\n",
    "torch.nn.AvgPool1d\n",
    "torch.nn.AvgPool2d\n",
    "torch.nn.AvgPool3d\n",
    "torch.nn.FractionalMaxPool2d\n",
    "torch.nn.LPPool1d\n",
    "torch.nn.LPPool2d\n",
    "torch.nn.AdaptiveMaxPool1d\n",
    "torch.nn.AdaptiveMaxpool2d\n",
    "torch.nn.AdaptiveMaxPool3d\n",
    "torch.nn.AdaptiveAvgPool1d\n",
    "torch.nn.AdaptiveAvgppol2d\n",
    "torch.nn.AdaptiveAvgPool3d\n",
    "\n",
    "Padding Layers\n",
    "\n",
    "torch.nn.ReflectionPad1d\n",
    "torch.nn.Reflectionpad2d\n",
    "torch.nn.ReplicationPad1d\n",
    "torch.nn.ReplicationPad2d\n",
    "torch.nn.ReplicationPad3d\n",
    "torch.nn.ZeroPad2d\n",
    "torch.nn.ConstantPad1d\n",
    "torch.nn.ConstantPad2d\n",
    "torch.nn.ConstantPad3d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e933b3-9774-483b-b8ef-bc970a1ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.\n",
    "Non-linear activations (weighted sum, non-linearity)\n",
    "1) torch.nn.ELU\n",
    "It will use to apply the element-wise function:\n",
    "ELU(x)=max(0,x)+min(0,α*(exp(x)-1))\n",
    "2) torch.nn.Hardshrink\n",
    "It will use to apply the hard shrinkage function element-wise function:\n",
    "torch.nn in PyTorch\n",
    "3) torch.nn.LeakyReLU\n",
    "It will use to apply the element-wise function:\n",
    "LeakyReLu(x)=max(0,x) +negative_slope*min(0,x)\n",
    "4) torch.nn.LogSigmoid\n",
    "It will use to apply the element-wise function:\n",
    "torch.nn in PyTorch\n",
    "5) torch.nn.MultiheadAttention\n",
    "It is used to allow the model to attend to information from different representation subspaces\n",
    "6) torch.nn.PReLU\n",
    "It will be used to apply the element-wise function:\n",
    "PReLU(x)=max(0,x)+a*min(0,x)\n",
    "7) torch.nn.ReLU\n",
    "It will use to apply the rectified linear unit function element-wise:\n",
    "ReLU(x)=max(0,x)\n",
    "8) torch.nn.ReLU6\n",
    "It will be used to apply the element-wise function:\n",
    "ReLU6(x)=min(max(0,x),6)\n",
    "9) torch.nn.RReLU\n",
    "It will use to apply the randomized leaky rectified linear unit function, element-wise, as described in the paper:\n",
    "torch.nn in PyTorch\n",
    "10) torch.nn.SELU\n",
    "It will use to apply the element-wise function as:\n",
    "SELU(x)=scale*(max(0,x)+ min(0,a*(exp(x)-1)))\n",
    "\n",
    "Here α= 1.6732632423543772848170429916717 and scale = 1.0507009873554804934193349852946.\n",
    "11) torch.nn.CELU\n",
    "It will use to apply the element-wise function as:\n",
    "torch.nn in PyTorch\n",
    "12) torch.nn.Sigmoid\n",
    "It will use to apply the element-wise function as:\n",
    "torch.nn in PyTorch\n",
    "13) torch.nn.Softplus\n",
    "It will use to apply the element-wise function as:\n",
    "torch.nn in PyTorch\n",
    "14) torch.nn.Softshrink\n",
    "It will use to apply soft shrinkage function elementwise as:\n",
    "torch.nn in PyTorch\n",
    "15) torch.nn.Softsign\n",
    "It will use to apply the element-wise function as:\n",
    "torch.nn in PyTorch\n",
    "16) torch.nn.Tanh\n",
    "It will use to apply the element-wise function as:\n",
    "torch.nn in PyTorch\n",
    "17) torch.nn.Tanhshrink\n",
    "It will use to apply the element-wise function as:\n",
    "Tanhshrink(x)=x-Tanh(x)\n",
    "18) torch.nn.Threshold\n",
    "It will use to thresholds each element of the input Tensor. Threshold is defined as:\n",
    "torch.nn in PyTorch\n",
    "7.\n",
    "Non-linear activations (other)\n",
    "1) torch.nn.Softmin\n",
    "It is used to apply the softmin function to an n-dimensional input Tensor to rescaling them. After that, the elements of the n-dimensional output Tensor lies in the range 0, 1, and sum to 1. Softmin is defined as:\n",
    "torch.nn in PyTorch\n",
    "2) torch.nn.Softmax\n",
    "It is used to apply the softmax function to an n-dimensional input Tensor to rescaling them. After that, the elements of the n-dimensional output Tensor lies in the range 0, 1, and sum to 1. Softmax is defined as:\n",
    "torch.nn in PyTorch\n",
    "3) torch.nn.Softmax2d\n",
    "It is used to apply SoftMax over features to each spatial location.\n",
    "4) torch.nn.LogSoftmax\n",
    "It is used to apply LogSoftmax function to an n-dimensional input Tensor. The LofSoftmax function can be defined as:\n",
    "torch.nn in PyTorch\n",
    "5) torch.nn.AdaptiveLogSoftmaxWithLoss\n",
    "It is a strategy for training models with large output spaces. It is very effective when the label distribution is highly imbalanced\n",
    "8.\n",
    "Normalization layers\n",
    "1) torch.nn.BatchNorm1d\n",
    "It is used to apply batch normalization over a 2D or 3D inputs.\n",
    "torch.nn in PyTorch\n",
    "2) torch.nn.BatchNorm2d\n",
    "It is used to apply batch normalization over a 4D.\n",
    "torch.nn in PyTorch\n",
    "3) torch.nn.BatchNorm3d\n",
    "It is used to apply batch normalization over 5D inputs.\n",
    "torch.nn in PyTorch\n",
    "4) torch.nn.GroupNorm\n",
    "It is used to apply group normalization over a mini-batch of inputs.\n",
    "torch.nn in PyTorch\n",
    "5) torch.nn.SyncBatchNorm\n",
    "It is used to apply batch normalization over n-dimensional inputs.\n",
    "torch.nn in PyTorch\n",
    "6) torch.nn.InstanceNorm1d\n",
    "It is used to apply an instance normalization over a 3D input.\n",
    "torch.nn in PyTorch\n",
    "7) torch.nn.InstanceNorm2d\n",
    "It is used to apply an instance normalization over a 4D input.\n",
    "torch.nn in PyTorch\n",
    "8) torch.nn.InstanceNorm3d\n",
    "It is used to apply an instance normalization over a 5D input.\n",
    "torch.nn in PyTorch\n",
    "9) torch.nn.LayerNorm\n",
    "It is used to apply layer normalization over a mini-batch of inputs.\n",
    "torch.nn in PyTorch\n",
    "10) torch.nn.LocalResponseNorm\n",
    "It is used to apply local response normalization over an input signal which is composed of several input planes, where the channel occupies the second dimension.\n",
    "9.\n",
    "Recurrent layers\n",
    "1) torch.nn.RNN\n",
    "It is used to apply a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "ht=tanh(Wih xt+bih+Whh tt-1+bhh)\n",
    "2) torch.nn.LSTM\n",
    "It is used to apply a multi-layer long short-term memory (LSTM) RNN to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "torch.nn in PyTorch\n",
    "3) torch.nn.GRU\n",
    "It is used to apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "torch.nn in PyTorch\n",
    "4) torch.nn.RNNCell\n",
    "It is used to apply an Elman RNN cell with tanh or ReLU non-linearity to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "h'=tanh(Wih x+bih+Whh h+bhh)\n",
    "ReLU is used in place of tanh\n",
    "5) torch.nn.LSTMCell\n",
    "It is used to apply a long short-term memory (LSTM) cell to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "torch.nn in PyTorch\n",
    "Where σ is the sigmoid function, and * is the Hadamard product.\n",
    "6) torch.nn.GRUCell\n",
    "It is used to apply a gated recurrent unit (GRU) cell to an input sequence. Each layer computes the following function for each element in the input sequence:\n",
    "torch.nn in PyTorch\n",
    "10.\n",
    "Linear layers\n",
    "1) torch.nn.Identity\n",
    "It is a placeholder identity operator which is argument-insensitive.\n",
    "2) torch.nn.Linear\n",
    "It is used to apply a linear transformation to the incoming data:\n",
    "y=xAT+b\n",
    "3) torch.nn.Bilinear\n",
    "It is used to apply a bilinear transformation to the incoming data:\n",
    "y=x1 Ax2+b\n",
    "11.\n",
    "Dropout layers\n",
    "1) torch.nn.Dropout\n",
    "It is used for regularization and prevention of co-adaptation of neurons. A factor oftorch.nn in PyTorchduring training scales the output. That means the module computes an identity function during the evaluation.\n",
    "2) torch.nn.Dropout2d\n",
    "If adjacent pixels within feature maps are correlated, then torch.nn.Dropout will not regularize the activations, and it will decrease the effective learning rate. In this case, torch.nn.Dropout2d() is used to promote independence between feature maps.\n",
    "3) torch.nn.Dropout3d\n",
    "If adjacent pixels within feature maps are correlated, then torch.nn.Dropout will not regularize the activations, and it will decrease the effective learning rate. In this case, torch.nn.Dropout2d () is used to promote independence between feature maps.\n",
    "4) torch.nn.AlphaDropout\n",
    "It is used to apply Alpha Dropout over the input. Alpha Dropout is a type of Dropout which maintains the self-normalizing property.\n",
    "12.\n",
    "Sparse layers\n",
    "1) torch.nn.Embedding\n",
    "It is used to store word embeddings and retrieve them using indices. The input for the module is a list of indices, \n",
    "and the output is the corresponding word embedding.\n",
    "2) torch.nn.EmbeddingBag\n",
    "It is used to compute sums or mean of bags of embedding without instantiating the Intermediate embedding.\n",
    "13.\n",
    "Distance Function\n",
    "1) torch.nn.CosineSimilarity\n",
    "It will return the cosine similarity between x1 and x2, computed along dim.\n",
    "torch.nn in PyTorch\n",
    "2) torch.nn.PairwiseDistance\n",
    "It computes the batch-wise pairwise distance between vectors v1, v2 using the p-norm:\n",
    "torch.nn in PyTorch\n",
    "14.\n",
    "Loss function\n",
    "1) torch.nn.L1Loss\n",
    "It is used to a criterion which measures the mean absolute error between each element in the input x and target y. The unreduced loss can be described as:\n",
    "      l(x,y)=L={l1,...,ln },ln=|xn-yn |,\n",
    "Where N is the batch size.\n",
    "2) torch.nn.MSELoss\n",
    "It is used to a criterion which measures the mean squared error between each element in the input x and target y. The unreduced loss can be described as:\n",
    "l(x,y)=L={l1,...,ln },ln=(xn-yn)2,\n",
    "Where N is the batch size.\n",
    "3) torch.nn.CrossEntropyLoss\n",
    "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is helpful when we train a classification problem with C classes.\n",
    "4) torch.nn.CTCLoss\n",
    "The Connectionist Temporal Classification loss calculates loss between a continuous time series and a target sequence.\n",
    "5) torch.nn.NLLLoss\n",
    "The Negative Log-Likelihood loss is used to train a classification problem with C classes.\n",
    "6) torch.nn.PoissonNLLLoss\n",
    "The Negative log-likelihood loss with the Poisson distribution of t\n",
    "target~Poisson(input)loss(input,target)=input-target*log(target!)he target.\n",
    "7) torch.nn.KLDivLoss\n",
    "It is a useful distance measure for continuous distribution, and it is also useful when we perform direct regression over the space of continuous output distribution.\n",
    "8) torch.nn.BCELoss\n",
    "It is used to create a criterion which measures the Binary Cross Entropy between the target and the output. The unreduced loss can be described as:\n",
    "l(x,y)=L={l1,...,ln },ln=-wn [yn*logxn+ (1-yn )*log(1-xn)],\n",
    "Where N is the batch size.\n",
    "9) torch.nn.BCEWithLogitsLoss\n",
    "It combines a Sigmoid layer and the BCELoss in one single class. We can take advantage of the log-sum-exp trick for numerical stability by combining the operation into one layer.\n",
    "10) torch.nn.MarginRankingLoss\n",
    "It creates a criterion which measures the loss of given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y which contain 1 or -1. The loss function for each sample in the mini-batch is as follows:\n",
    "      loss(x,y)=max(0,-y*(x1-x2 )+margin\n",
    "11) torch.nn.HingeEmbeddingLoss\n",
    "HingeEmbeddingLoss measures the loss of given an input tensor x and a labels tensor y which contain 1 or -1. It is used for measuring whether two inputs are similar or dissimilar. The loss function is defined as:\n",
    "torch.nn in PyTorch\n",
    "12) torch.nn.MultiLabelMarginLoss\n",
    "It is used to create a criterion which optimizes a multi-class multi-classification hinge loss between input x and output y.\n",
    "torch.nn in PyTorch\n",
    "13) torch.nn.SmoothL1Loss\n",
    "It is used to create a criterion which uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is also known as Huber loss:\n",
    "torch.nn in PyTorch\n",
    "14) torch.nn.SoftMarginLoss\n",
    "It is used to create a criterion which optimizes the two-class classification logistic loss between input tensor x and target tensor y which contain 1 or -1.\n",
    "torch.nn in PyTorch\n",
    "15) torch.nn.MultiLabelSoftMarginLoss\n",
    "It is used to create a criterion which optimizes the multi-label one-versus-all loss based on max-entropy between input x and target y of size (N, C).\n",
    "torch.nn in PyTorch\n",
    "16) torch.nn.CosineEmbeddingLoss\n",
    "It is used to create a criterion which measures the loss of given input tensors x1, x2 and a tensor label y with values 1 or -1. It is used for measuring whether two inputs are similar or dissimilar, using the cosine distance.\n",
    "torch.nn in PyTorch\n",
    "17) torch.nn.MultiMarginLoss\n",
    "It is used to create a criterion which optimizes a multi-class classification hinge loss between input x and output y.\n",
    "torch.nn in PyTorch\n",
    "18) torch.nn.TripletMarginLoss\n",
    "It is used to create a criterion which measures the triplet loss of given an input tensors x1, x2, x3 and a margin with a value greater than 0. It is used for measuring a relative similarity between samples. A triplet is composed of an anchor, positive example, and a negative example.\n",
    "L(a,p,n)=max{d(ai,pi )-d(ai,ni )+margin,0}\n",
    "15.\n",
    "Vision layers\n",
    "1) torch.nn.PixelShuffle\n",
    "It is used to re-arrange the elements in a tensor of shape(*,C×r2,H,W) to a tensor of shape (*,C,H×r,W,r)\n",
    "2) torch.nn.Upsample\n",
    "It is used to upsample a given multi-channel 1D, 2D or 3D data.\n",
    "3) torch.nn.upsamplingNearest2d\n",
    "It is used to apply 2D nearest neighbor upsampling to an input signal which is composed with multiple input channel.\n",
    "4) torch.nn.UpsamplingBilinear2d\n",
    "It is used to apply 2D bilinear upsampling to an input signal which is composed with, multiple input channel.\n",
    "16.\n",
    "DataParallel layers(multi-GPU, distributed)\n",
    "1) torch.nn.DataParallel\n",
    "It is used to implement data parallelism at the module level.\n",
    "2) torch.nn.DistributedDataParallel\n",
    "It is used to implement distributed data parallelism, which is based on the torch.distributed package at the module level.\n",
    "3) torch.nn.DistributedDataParallelCPU\n",
    "It is used to implement distributed data parallelism for the CPU at the module level.\n",
    "17.\n",
    "Utilities\n",
    "1) torch.nn.clip_grad_norm_\n",
    "It is used to clip the gradient norm of an iterable of parameters.\n",
    "2) torch.nn.clip_grad_value_\n",
    "It is used to clip the gradient norm of an iterable of parameters at the specified value.\n",
    "3) torch.nn.parameters_to_vector\n",
    "It is used to convert parameters to one vector.\n",
    "4) torch.nn.vector_to_parameters\n",
    "It is used to convert one vector to the parameters.\n",
    "5) torch.nn.weight_norm\n",
    "It is used to apply weight normalization to a parameter in the given module.\n",
    "torch.nn in PyTorch\n",
    "6) torch.nn.remove_weight_norm\n",
    "It is used to remove the weight normalization and re-parameterization from a module.\n",
    "7) torch.nn.spectral_norm\n",
    "It is used to apply spectral normalization to a parameter in the given module.\n",
    "8) torch.nn.PackedSequence\n",
    "It will use to hold the data and list of batch_sizes of a packed sequence.\n",
    "9) torch.nn.pack_padded_sequence\n",
    "It is used to pack a Tensor containing padded sequences of variable length.\n",
    "10) torch.nn.pad_packed_sequence\n",
    "It is used to pads a packed batch of variable-length sequences.\n",
    "11) torch.nn.pad_sequence\n",
    "It is used to pad a list of variable length Tensors with padding value.\n",
    "12) torch.nn.pack_sequence\n",
    "It is used to packs a list of variable length Tensors\n",
    "13) torch.nn.remove_spectral_norm\n",
    "It is used to removes the spectral normalization and re-parameterization from a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb49212-cc89-4152-9c06-d4d0177553c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now we will go with the basics of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a07ce3-fa52-4379-823f-5ffd301e3d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition tensor([50., 70., 90.])\n",
      "Subtraction tensor([30., 30., 30.])\n",
      "Multiplication tensor([ 400., 1000., 1800.])\n",
      "Division tensor([4.0000, 2.5000, 2.0000])\n",
      "Matrix Multiplication:\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([10,20,30],dtype=torch.float32)\n",
    "b=torch.tensor([40,50,60],dtype=torch.float32)\n",
    "c=a+b\n",
    "print(\"Addition\",c)\n",
    "d=b-a\n",
    "print(\"Subtraction\",d)\n",
    "e=a*b\n",
    "print(\"Multiplication\",e)\n",
    "f=b/a\n",
    "print(\"Division\",f)\n",
    "g=torch.tensor([[1,2],[3,4]])\n",
    "h=torch.tensor([[5,6],[7,8]])\n",
    "i=torch.matmul(g,h)\n",
    "print(\"Matrix Multiplication:\\n\",i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
