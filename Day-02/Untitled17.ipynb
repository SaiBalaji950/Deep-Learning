{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c2e13-cc18-4c9f-819a-178b5117be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keras\n",
    "\n",
    "Keras is a high level API(Application Programming Interface) and training deep learning models and\n",
    "it is a part of tensorflow and makes it is easy to design neural networks.\n",
    "\n",
    "Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK. \n",
    "It was developed by one of the Google engineers, Francois Chollet. It is made user-friendly, extensible, and modular for facilitating faster\n",
    "experimentation with deep neural networks. It not only supports Convolutional Networks and Recurrent Networks individually but also their combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70391f3a-d62d-46e7-b7c7-b6e39ccedc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights\n",
    "\n",
    "weights are the parameters in the neural networks to determine the strength and directions \n",
    "of the connection between neurons.\n",
    "\n",
    "Weights are initialized randomly and updated during training using algorithms.\n",
    "the goal of the weights to minimize the loss function, which measures the error between \n",
    "predicted values and actual values.\n",
    "\n",
    "Input layer:\n",
    "\n",
    "The Input layer is simply passes the raw data to hidden layer.\n",
    "its size is depends on number of features or data dimensions. \n",
    "\n",
    "Hidden Layer:\n",
    "\n",
    "The Hidden Layer is the intermediate layer between input and output layer.\n",
    "Why hidden means beacause thier values is not visible in the training data or final output.\n",
    "These layers extract features and learn patterns from data.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "Output Layer is the final layer in the neural network weights which is responsible for the producing the models predictions.\n",
    "\n",
    "weights: Z=W1X1+W2X2+....+WnXn+b\n",
    "\n",
    "w=weights, x=inputs and b=bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92bdc0-68a3-43b7-be6a-5a0269ca32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation Function\n",
    "\n",
    "Activation Function introduce non-linearity to the model, allowing it to learn complex patterns.\n",
    "without them, the network would act like alinear regression model, unable to capture the non-linear relationship.\n",
    "\n",
    "there are six types of activation functions, they are:\n",
    "1) Linear Activation\n",
    "2) Sigmoid\n",
    "3) Relu(Rectified Linear Unit)\n",
    "4) Leaky Relu\n",
    "5) Tanh(Hyperbolic Tangent)\n",
    "6) Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f83a8-176f-4021-8859-14dc7839539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Activation\n",
    "\n",
    "f(x)=x\n",
    "it is rarely used since it doesn't introduce non-linearity.\n",
    "\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "f(x)=(1/(1+e^-x))\n",
    "issue: can saturate(gradient vanishes for very small/large units).\n",
    "\n",
    "Relu(Rectified Linear Unit)\n",
    "\n",
    "Default activation for hidden layers.\n",
    "computationally efficient and mitigates vanishing gradient issues.\n",
    "can cause dead neurons when weights drive x<0\n",
    "\n",
    "Leaky Relu\n",
    "\n",
    "f(x)=x if x>0,else 0.01x\n",
    "similar to relu but allows small gradients for negative points.\n",
    "\n",
    "Tanh(Hyperbolic Tangent)\n",
    "f(x)=((e^x-e^-x)/(e^x+e^-x))\n",
    "works well in hidden layers to center data around zero.\n",
    "cause: can also suffer from vanishing gradient.\n",
    "\n",
    "Softmax:\n",
    "f(xi)= ((e^xi/âˆ‘j^(e^xi)) \n",
    "Multiple Classification Problems to output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3e290-a040-464b-ba7d-46d76ff1a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keras Layers\n",
    "\n",
    "Layers are the computational units in keras. They Process inputs and produce output using weights and Activation\n",
    "Functions. Keras Provides a variety of layers to build different types of neural networks.\n",
    "\n",
    "there are 6 types of layers in keras\n",
    "\n",
    "1) Dense(Fully Connected Layer)\n",
    "2) Conv2D (Convolutional Layer)\n",
    "3) MaxPooling2D\n",
    "4) Flatten\n",
    "5) Dropout \n",
    "6) LSTM(Long-Short-Term-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2ec65-c20f-448f-88e4-d8b30d206e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense\n",
    "\n",
    "It is fully Connected Layer\n",
    "It is connected to every input neuron to every output neuron.\n",
    "It is Mostly used in the Feed-Forward Neural Networks.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import Dense\n",
    "Dense(64,activation='relu')  #64 neurons activating relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c99177-402b-49da-8ec1-4312ae7db532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv2D\n",
    "\n",
    "It is Used for Image Data.\n",
    "Extracting Spatial Features using Filters.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "Conv2D(filters=32,kernel_size=(3,3),activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe63b9-4a16-46b1-8783-71eafd7b6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPooling2D\n",
    "\n",
    "It Reduces the Spatial Dimensions of the feature maps.\n",
    "It is Mainly used for the Convolutional Layers.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "MaxPooling2D(pool_size=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac0b37-2142-434a-89f1-1fe45af2c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flatten\n",
    "\n",
    "Converts Multi-Dimensionality Data into 1D Vector for input into Dense Layers.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import Flatten\n",
    "Flatten() # Flattens input shape to (batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af7a31-fda2-4ed5-9257-547575831885",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout \n",
    "\n",
    "Randomly sets a fraction of input to zero during training to prevent overfitting.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import Dropout\n",
    "Droput(0.5) #50% of inputs were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d499b-c491-4941-a69e-7bd957512b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM(Long-Term-Short Memory)\n",
    "\n",
    "It is used for the Sequential Data like time or text series.\n",
    "Maintains Long-Term Dependencies.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import LSTM\n",
    "LSTM(128) #128 memory unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554c345-1063-4f35-8fd9-173b0abd61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keras Models\n",
    "\n",
    "1) Sequential \n",
    "2) Functional API\n",
    "3) Model Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8eb30a-97b1-4ee5-8c49-cd44f570ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequential Model\n",
    "\n",
    "A Linear Stock of Layers.\n",
    "Best for simple Architectures where layers flow Sequentially.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras,layers import Dense\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu',input_shape=(10,)),#Input Layer\n",
    "    Dense(32,activation='relu'),#Hidden Layer\n",
    "    Dense(1,activation='sigmoid')]) #Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b3ddb-4237-4475-a57d-dfbe5238528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Functional API\n",
    "\n",
    "For the more Complex architectures, like multi-input/output or shared layers.\n",
    "Allows more Flexibility compared to the sequential mode;.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "input_layer=Input(shape=(10,))\n",
    "hidden_layer=Dense(64,activation='relu')(input_layer)\n",
    "output_layer=Dense(1,activation='sigmoid')(hidden_layer)\n",
    "model=Model(inputs=input_layer,outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87fd6e-2e18-47be-b822-609197d97127",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Subclassing\n",
    "\n",
    "offers full control and customization by subclassing the model class.\n",
    "suitable for advanced use cases.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layer import Dense\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.dense1=Dense(64,activation='relu')\n",
    "        self.dense2=Dense(1,activation='sigmoid')\n",
    "    def call(self,inputs):\n",
    "        x=self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "model=MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15ea63-fd19-46f0-8538-916d34507750",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Workflow\n",
    "\n",
    "1) Compile the model\n",
    "\n",
    "prepare the model by specific key components\n",
    "\n",
    "optimizer: controls how the model updates weights based on the loss.\n",
    "in optimizer mainly we used adam(adaptive moment estimation) which is efficient optimizer.\n",
    "\n",
    "loss function: Measures the difference between predicted and true values \n",
    "here in loss function we used binary_crossentropy for binary classifications.\n",
    "\n",
    "metrics: we used accuracy in this model for prediction.\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b155c74-79d5-4f19-a38c-b66cc613f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Train the model\n",
    "\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=32)\n",
    "\n",
    "Here x_train,y_train means training of input variables and output variables of the dataset.\n",
    "epochs means is comepletely pass through the entire dataset that means here the model goes \n",
    "through the dataset 10 times. Here batch_size means The Number of Training examples Processed \n",
    "before updating the model weights. here dataset is divided into smaller parts of 32 samples.\n",
    "\n",
    "The Model Learns in Increments(batches) over multiple passes(epochs).\n",
    "After each batch, the optimizer updates the weights to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ad45a-55d6-438e-9fda-ce90ca82c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Evaluate the Model\n",
    "\n",
    "model.evaluate(x_test,y_test)\n",
    "\n",
    "4) Model Predictions\n",
    "\n",
    "model.predict(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
