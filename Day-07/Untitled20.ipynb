{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633bc6-963d-4040-97ef-7f627fd5da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNet\n",
    "\n",
    "It was invented in 2017.\n",
    "DenseNet connects the each layer to every other layer follows the feedforward fashion,creating a Dense block.\n",
    "This ensures that feature maps from all previous layers are used as input for subsequent layers, improving the flow of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f35508c-1dda-4664-b8dc-f9d6bcb4aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 339s 541ms/step - loss: 1.9113 - accuracy: 0.2721 - val_loss: 1.6904 - val_accuracy: 0.3685\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 336s 537ms/step - loss: 1.6292 - accuracy: 0.3979 - val_loss: 1.6844 - val_accuracy: 0.3848\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 336s 537ms/step - loss: 1.5062 - accuracy: 0.4463 - val_loss: 1.4691 - val_accuracy: 0.4564\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 338s 540ms/step - loss: 1.4244 - accuracy: 0.4796 - val_loss: 1.3893 - val_accuracy: 0.4869\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 337s 539ms/step - loss: 1.3526 - accuracy: 0.5078 - val_loss: 1.3146 - val_accuracy: 0.5235\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 337s 539ms/step - loss: 1.2899 - accuracy: 0.5357 - val_loss: 1.2676 - val_accuracy: 0.5455\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 329s 527ms/step - loss: 1.2410 - accuracy: 0.5523 - val_loss: 1.2239 - val_accuracy: 0.5493\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 271s 434ms/step - loss: 1.1973 - accuracy: 0.5688 - val_loss: 1.1952 - val_accuracy: 0.5610\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 272s 436ms/step - loss: 1.1605 - accuracy: 0.5839 - val_loss: 1.1477 - val_accuracy: 0.5925\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 280s 448ms/step - loss: 1.1274 - accuracy: 0.5968 - val_loss: 1.1324 - val_accuracy: 0.6040\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 1.1322 - accuracy: 0.5980\n",
      "Loss: 1.1322485208511353,Accuracy: 0.5979999899864197\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def dense(x,num_convs,growth_rate):\n",
    "    for _ in range(num_convs):\n",
    "        x1=layers.Conv2D(growth_rate,(3,3),padding='same',activation='relu')(x)\n",
    "        x=layers.concatenate([x,x1],axis=-1)\n",
    "    return x\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=dense(x,4,32)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=64,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss},Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c1c105-5676-417b-ba22-0383f44ee59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 335s 446ms/step - loss: 1.1063 - accuracy: 0.6202 - val_loss: 0.6230 - val_accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 338s 451ms/step - loss: 0.4139 - accuracy: 0.8789 - val_loss: 0.3199 - val_accuracy: 0.9121\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 355s 473ms/step - loss: 0.2882 - accuracy: 0.9164 - val_loss: 0.2295 - val_accuracy: 0.9350\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 344s 458ms/step - loss: 0.2245 - accuracy: 0.9351 - val_loss: 0.1795 - val_accuracy: 0.9498\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 336s 448ms/step - loss: 0.1914 - accuracy: 0.9447 - val_loss: 0.1751 - val_accuracy: 0.9513\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 0.1643 - accuracy: 0.9520\n",
      "Loss: 0.16431160271167755, Accuracy: 0.9520000219345093\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_train=tf.image.grayscale_to_rgb(tf.image.resize(x_train[...,tf.newaxis],(32,32))).numpy()\n",
    "x_test=tf.image.grayscale_to_rgb(tf.image.resize(x_test[...,tf.newaxis],(32,32))).numpy()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def dense(x,num_convs,growth_rate):\n",
    "    for _ in range(num_convs):\n",
    "        x1=layers.Conv2D(growth_rate,(3,3),padding='same',activation='relu')(x)\n",
    "        x=layers.concatenate([x,x1],axis=-1)\n",
    "    return x\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=dense(x,4,32)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=5,batch_size=64,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8e4a2-a422-45e5-a63b-c32c7b830706",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hence these are the examples for the DenseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9716-0bf5-43bf-a12a-1455630c7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception\n",
    "\n",
    "It was invented in 2017.\n",
    "Xception is based on the depthwise separable convolutional layers which factorized the\n",
    "standard convolution into two layers: depthwise convolutional and pointwise convolutional.\n",
    "This reduces the number of parameters and computations, making the model more efficient.\n",
    "This model is known for its excellent performance in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240995a9-00e7-4de7-a5d2-a0384a69502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "477/477 [==============================] - 165s 344ms/step - loss: 2.0226 - accuracy: 0.2342 - val_loss: 1.8776 - val_accuracy: 0.3080\n",
      "Epoch 2/5\n",
      "477/477 [==============================] - 172s 361ms/step - loss: 1.7743 - accuracy: 0.3255 - val_loss: 1.7692 - val_accuracy: 0.3109\n",
      "Epoch 3/5\n",
      "477/477 [==============================] - 180s 377ms/step - loss: 1.6575 - accuracy: 0.3780 - val_loss: 1.6086 - val_accuracy: 0.3942\n",
      "Epoch 4/5\n",
      "477/477 [==============================] - 178s 374ms/step - loss: 1.5962 - accuracy: 0.4123 - val_loss: 1.5419 - val_accuracy: 0.4259\n",
      "Epoch 5/5\n",
      "477/477 [==============================] - 178s 374ms/step - loss: 1.5284 - accuracy: 0.4437 - val_loss: 1.4986 - val_accuracy: 0.4478\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 1.4924 - accuracy: 0.4517\n",
      "Loss: 1.4924005270004272, Accuracy: 0.45170000195503235\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def xception(x,filters):\n",
    "    shortcut=x\n",
    "    x=layers.SeparableConv2D(filters,(3,3),padding='same',activation='relu')(x)\n",
    "    x=layers.SeparableConv2D(filters,(3,3),padding='same',activation=None)(x)\n",
    "    shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "    x=layers.add([x,shortcut])\n",
    "    x=layers.ReLU()(x)\n",
    "    return x\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=xception(x,64)\n",
    "x=xception(x,128)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=5,batch_size=84,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3190c95d-38c9-460f-ac55-85ae7dd71b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "572/572 [==============================] - 222s 387ms/step - loss: 1.6474 - accuracy: 0.3859 - val_loss: 1.2270 - val_accuracy: 0.5922\n",
      "Epoch 2/5\n",
      "572/572 [==============================] - 234s 409ms/step - loss: 0.8324 - accuracy: 0.7342 - val_loss: 0.5487 - val_accuracy: 0.8341\n",
      "Epoch 3/5\n",
      "572/572 [==============================] - 252s 440ms/step - loss: 0.5061 - accuracy: 0.8440 - val_loss: 0.3752 - val_accuracy: 0.8887\n",
      "Epoch 4/5\n",
      "572/572 [==============================] - 251s 439ms/step - loss: 0.3930 - accuracy: 0.8820 - val_loss: 0.2978 - val_accuracy: 0.9142\n",
      "Epoch 5/5\n",
      "572/572 [==============================] - 251s 440ms/step - loss: 0.3292 - accuracy: 0.9012 - val_loss: 0.2826 - val_accuracy: 0.9172\n",
      "313/313 [==============================] - 19s 59ms/step - loss: 0.2603 - accuracy: 0.9214\n",
      "Loss: 0.26027387380599976, Accuracy: 0.9214000105857849\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_train=tf.image.grayscale_to_rgb(tf.image.resize(x_train[...,tf.newaxis],(32,32))).numpy()\n",
    "x_test=tf.image.grayscale_to_rgb(tf.image.resize(x_test[...,tf.newaxis],(32,32))).numpy()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def exception(x,filters):\n",
    "    shortcut=x\n",
    "    x=layers.SeparableConv2D(filters,(3,3),padding='same',activation='relu')(x)\n",
    "    x=layers.SeparableConv2D(filters,(3,3),padding='same',activation='relu')(x)\n",
    "    shortcut=layers.Conv2D(filters,(1,1),padding='same',activation='relu')(shortcut)\n",
    "    x=layers.add([x,shortcut])\n",
    "    return x\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=exception(x,64)\n",
    "x=exception(x,128)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=5,batch_size=84,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfc291-c1b1-4082-829d-f12a360eb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNet\n",
    "\n",
    "it was invented in 2017.\n",
    "MobileNet uses depthwise separable convolutions, similar to the Xception, to reduce computational complexity.\n",
    "It is a highly efficient model designed for mobile and embedded vision applications.\n",
    "The Model is lightweight but still acheives the high performance in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738dccc8-4cb9-4e12-87bc-5a148ddfdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "477/477 [==============================] - 35s 72ms/step - loss: 2.1407 - accuracy: 0.1947 - val_loss: 2.0169 - val_accuracy: 0.2670\n",
      "Epoch 2/5\n",
      "477/477 [==============================] - 34s 71ms/step - loss: 1.9364 - accuracy: 0.2752 - val_loss: 1.8719 - val_accuracy: 0.3057\n",
      "Epoch 3/5\n",
      "477/477 [==============================] - 35s 72ms/step - loss: 1.8398 - accuracy: 0.3097 - val_loss: 1.7908 - val_accuracy: 0.3266\n",
      "Epoch 4/5\n",
      "477/477 [==============================] - 36s 76ms/step - loss: 1.7755 - accuracy: 0.3292 - val_loss: 1.7470 - val_accuracy: 0.3424\n",
      "Epoch 5/5\n",
      "477/477 [==============================] - 36s 75ms/step - loss: 1.7339 - accuracy: 0.3469 - val_loss: 1.7085 - val_accuracy: 0.3582\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7046 - accuracy: 0.3553\n",
      "Loss: 1.7046306133270264, Accuracy: 0.35530000925064087\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=layers.DepthwiseConv2D((3,3),padding='same',activation='relu')(x)\n",
    "x=layers.Conv2D(64,(1,1),padding='same',activation='relu')(x)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=5,batch_size=84,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5472dd3-1140-499d-8004-025f0f70bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 43s 56ms/step - loss: 1.8480 - accuracy: 0.3077 - val_loss: 1.5164 - val_accuracy: 0.4272\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 42s 56ms/step - loss: 1.4446 - accuracy: 0.4743 - val_loss: 1.3400 - val_accuracy: 0.5213\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 42s 56ms/step - loss: 1.3078 - accuracy: 0.5464 - val_loss: 1.2101 - val_accuracy: 0.6067\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 42s 56ms/step - loss: 1.1950 - accuracy: 0.6080 - val_loss: 1.0945 - val_accuracy: 0.6672\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 42s 57ms/step - loss: 1.0784 - accuracy: 0.6624 - val_loss: 0.9755 - val_accuracy: 0.7005\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 42s 57ms/step - loss: 0.9687 - accuracy: 0.6996 - val_loss: 0.8995 - val_accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 42s 57ms/step - loss: 0.8786 - accuracy: 0.7282 - val_loss: 0.8049 - val_accuracy: 0.7544\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 43s 57ms/step - loss: 0.8081 - accuracy: 0.7506 - val_loss: 0.7380 - val_accuracy: 0.7758\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 43s 57ms/step - loss: 0.7509 - accuracy: 0.7671 - val_loss: 0.7101 - val_accuracy: 0.7828\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 43s 57ms/step - loss: 0.7076 - accuracy: 0.7812 - val_loss: 0.6611 - val_accuracy: 0.7954\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6515 - accuracy: 0.7988\n",
      "Loss: 0.6515362858772278, Accuracy: 0.35530000925064087\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_train=tf.image.grayscale_to_rgb(tf.image.resize(x_train[...,tf.newaxis],(32,32))).numpy()\n",
    "x_test=tf.image.grayscale_to_rgb(tf.image.resize(x_test[...,tf.newaxis],(32,32))).numpy()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=layers.DepthwiseConv2D((3,3),padding='same',activation='relu')(x)\n",
    "x=layers.Conv2D(64,(1,1),padding='same',activation='relu')(x)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=64,validation_split=0.2)\n",
    "loss,accurcay=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4dd77-aea2-4a24-ae7f-49610e695e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hence it is known as the MobileNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6bf89-3b48-4512-a337-e352d28288be",
   "metadata": {},
   "outputs": [],
   "source": [
    "NasNet\n",
    "\n",
    "It was invented in 2017.\n",
    "NasNet(Neural Architecture Search Network) is the result of the Googles Neural Architecture Search(Nas)\n",
    "method, which automatically designed the best architecture for a given task.\n",
    "The Full NasNet architecture is very complex and large, but it can be simplified for small datasets like cifar10.\n",
    "NasNet Generally performs very well in the large scale tasks like image classification and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d52f2216-1dd5-45ef-99d0-77c86cef50e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "477/477 [==============================] - 213s 447ms/step - loss: 1.8045 - accuracy: 0.3197 - val_loss: 1.5332 - val_accuracy: 0.4396\n",
      "Epoch 2/10\n",
      "477/477 [==============================] - 208s 436ms/step - loss: 1.4287 - accuracy: 0.4794 - val_loss: 1.2947 - val_accuracy: 0.5364\n",
      "Epoch 3/10\n",
      "477/477 [==============================] - 202s 424ms/step - loss: 1.2400 - accuracy: 0.5509 - val_loss: 1.2188 - val_accuracy: 0.5677\n",
      "Epoch 4/10\n",
      "477/477 [==============================] - 202s 423ms/step - loss: 1.1184 - accuracy: 0.6004 - val_loss: 1.0446 - val_accuracy: 0.6272\n",
      "Epoch 5/10\n",
      "477/477 [==============================] - 198s 415ms/step - loss: 1.0320 - accuracy: 0.6339 - val_loss: 1.0636 - val_accuracy: 0.6168\n",
      "Epoch 6/10\n",
      "477/477 [==============================] - 253s 530ms/step - loss: 0.9653 - accuracy: 0.6579 - val_loss: 0.9733 - val_accuracy: 0.6564\n",
      "Epoch 7/10\n",
      "477/477 [==============================] - 201s 422ms/step - loss: 0.9234 - accuracy: 0.6758 - val_loss: 0.9424 - val_accuracy: 0.6670\n",
      "Epoch 8/10\n",
      "477/477 [==============================] - 204s 428ms/step - loss: 0.8762 - accuracy: 0.6903 - val_loss: 0.9066 - val_accuracy: 0.6774\n",
      "Epoch 9/10\n",
      "477/477 [==============================] - 200s 420ms/step - loss: 0.8371 - accuracy: 0.7040 - val_loss: 0.8590 - val_accuracy: 0.6958\n",
      "Epoch 10/10\n",
      "477/477 [==============================] - 200s 419ms/step - loss: 0.7893 - accuracy: 0.7246 - val_loss: 0.7885 - val_accuracy: 0.7250\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.8006 - accuracy: 0.7233\n",
      "Loss: 0.8006107211112976, Accuracy: 0.7232999801635742\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=layers.Conv2D(64,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.MaxPooling2D()(x)\n",
    "x=layers.Conv2D(128,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.Conv2D(256,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=84,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33118446-dbe0-47cf-bbe1-d91ca5c320d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "572/572 [==============================] - 213s 371ms/step - loss: 0.6351 - accuracy: 0.7908 - val_loss: 0.2244 - val_accuracy: 0.9322\n",
      "Epoch 2/10\n",
      "572/572 [==============================] - 214s 374ms/step - loss: 0.1587 - accuracy: 0.9528 - val_loss: 0.1031 - val_accuracy: 0.9696\n",
      "Epoch 3/10\n",
      "572/572 [==============================] - 224s 392ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.1013 - val_accuracy: 0.9668\n",
      "Epoch 4/10\n",
      "572/572 [==============================] - 231s 404ms/step - loss: 0.0873 - accuracy: 0.9730 - val_loss: 0.0604 - val_accuracy: 0.9822\n",
      "Epoch 5/10\n",
      "572/572 [==============================] - 240s 420ms/step - loss: 0.0724 - accuracy: 0.9782 - val_loss: 0.0518 - val_accuracy: 0.9828\n",
      "Epoch 6/10\n",
      "572/572 [==============================] - 242s 423ms/step - loss: 0.0625 - accuracy: 0.9806 - val_loss: 0.0886 - val_accuracy: 0.9719\n",
      "Epoch 7/10\n",
      "572/572 [==============================] - 244s 427ms/step - loss: 0.0536 - accuracy: 0.9839 - val_loss: 0.0401 - val_accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "572/572 [==============================] - 237s 415ms/step - loss: 0.0473 - accuracy: 0.9853 - val_loss: 0.0513 - val_accuracy: 0.9837\n",
      "Epoch 9/10\n",
      "572/572 [==============================] - 267s 467ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.0393 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "572/572 [==============================] - 273s 478ms/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 0.0444 - val_accuracy: 0.9862\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.0437 - accuracy: 0.9858\n",
      "Loss: 0.04372019320726395, Accuracy: 0.98580002784729\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_train=tf.image.grayscale_to_rgb(tf.image.resize(x_train[...,tf.newaxis],(32,32))).numpy()\n",
    "x_test=tf.image.grayscale_to_rgb(tf.image.resize(x_test[...,tf.newaxis],(32,32))).numpy()\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "input=layers.Input(shape=(32,32,3))\n",
    "x=layers.Conv2D(32,(3,3),padding='same',activation='relu')(input)\n",
    "x=layers.Conv2D(64,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.MaxPooling2D()(x)\n",
    "x=layers.Conv2D(128,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.Conv2D(256,(3,3),padding='same',activation='relu')(x)\n",
    "x=layers.GlobalAveragePooling2D()(x)\n",
    "x=layers.Dense(10,activation='softmax')(x)\n",
    "model=models.Model(inputs=input,outputs=x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=84,validation_split=0.2)\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86fc23-5b16-4aef-a3af-e10f27ed83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hence it is known as the NasNet(Neural Architecture Search Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db0c43-dfe0-4ee3-a377-dc722631828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNet\n",
    "\n",
    "It was invented in 2019.\n",
    "EfficientNet uses a compound scaling method to improve accuracy and efficiency by scaling the networks depth, width, and resolution at the same time.\n",
    "The model is highly efficient in terms of parameters, performance, and computational cost.\n",
    "EfficientNet models are known for achieving state-of-the-art performance on image classification tasks with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3132bd-6114-4dff-9f12-930012358562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "572/572 [==============================] - 87s 140ms/step - loss: 2.3160 - accuracy: 0.1124 - val_loss: 2.2995 - val_accuracy: 0.1716\n",
      "Epoch 2/5\n",
      "572/572 [==============================] - 69s 121ms/step - loss: 2.3066 - accuracy: 0.1202 - val_loss: 2.2994 - val_accuracy: 0.0996\n",
      "Epoch 3/5\n",
      "572/572 [==============================] - 69s 120ms/step - loss: 2.3011 - accuracy: 0.1283 - val_loss: 2.2961 - val_accuracy: 0.0958\n",
      "Epoch 4/5\n",
      "572/572 [==============================] - 69s 120ms/step - loss: 2.2969 - accuracy: 0.1298 - val_loss: 2.3033 - val_accuracy: 0.1087\n",
      "Epoch 5/5\n",
      "189/572 [========>.....................] - ETA: 37s - loss: 2.2937 - accuracy: 0.1320"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = tf.image.grayscale_to_rgb(tf.image.resize(x_train[..., tf.newaxis], (32, 32))).numpy()\n",
    "x_test = tf.image.grayscale_to_rgb(tf.image.resize(x_test[..., tf.newaxis], (32, 32))).numpy()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "base_model.trainable = False\n",
    "input_layer = layers.Input(shape=(32, 32, 3))\n",
    "x = base_model(input_layer)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(10, activation='softmax')(x)\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=84, validation_split=0.2)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80e017-b442-4a38-bfc3-b50826e41fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hence it is known as EfficientNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
